{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "declared-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import dask.array as da\n",
    "import dask.dataframe as df\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "under-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_object_method(obj):\n",
    "    object_methods = [method_name for method_name in dir(obj)\n",
    "                  if callable(getattr(obj, method_name))]\n",
    "    print(object_methods)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nervous-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_words(word_bag):    \n",
    "    split_words = word_bag.str.split()\n",
    "    stripped_words = split_words.flatten().map(lambda x: x.strip())\n",
    "    word_array = stripped_words.filter(lambda x: x!= \"\")\n",
    "    return word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invalid-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_words(words):\n",
    "    stopwords = open(\"data/stopwords.txt\", \"r\").read().split(\"\\n\")\n",
    "    return words.filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stone-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_document(document):\n",
    "    word_bag = db.read_text(document)\n",
    "    split_words = get_split_words(word_bag)\n",
    "    long_words = split_words.filter(lambda x: len(x) < 3)\n",
    "#     words_without_punctuations = long_words.map(puntuation_stripper)\n",
    "    return get_filtered_words(long_words)    \n",
    "#     return long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "choice-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDF(words_by_document):\n",
    "    unique_words = []\n",
    "    for words_for_single_document in words_by_document:\n",
    "        unique_words.append(words_for_single_document.distinct())\n",
    "    large_bag = db.concat(unique_words)\n",
    "    frequencies = large_bag.frequencies()\n",
    "    idf = frequencies.map(lambda x: (x[0], math.log(len(words_by_document)/x[1])))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "random-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_IDF(tf, idf):\n",
    "    tf = tf.frequencies()\n",
    "    tf_idf = tf.join(idf, lambda x: x[0], lambda x: x[0]).map(lambda x: (int(x[0][0], 16), x[0][1] * x[1][1]))\n",
    "    array_format = np.zeros(256)\n",
    "    for score in tf_idf:\n",
    "        array_format[score[0]] = score[1]\n",
    "    return [array_format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distributed-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"X_small_train.txt\"\n",
    "# set_name = \"X_train.txt\"\n",
    "url = 'https://storage.googleapis.com/uga-dsp/project1/files/' + set_name\n",
    "# url = \"data/train/\" + set_name\n",
    "file_names = pd.read_csv(url, header=None)[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facial-officer",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-69750adbec25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://storage.googleapis.com/uga-dsp/project1/data/bytes/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".bytes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     file_name = \"data/train/\" + file_name + \".bytes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mall_file_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_words_for_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-14fe16858e5f>\u001b[0m in \u001b[0;36mget_words_for_document\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlong_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     words_without_punctuations = long_words.map(puntuation_stripper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_filtered_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     return long_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6a730801bb27>\u001b[0m in \u001b[0;36mget_filtered_words\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filtered_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/stopwords.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/stopwords.txt'"
     ]
    }
   ],
   "source": [
    "all_file_words = []\n",
    "for file_name in file_names:\n",
    "    file_name = \"https://storage.googleapis.com/uga-dsp/project1/data/bytes/\" + file_name + \".bytes\"\n",
    "#     file_name = \"data/train/\" + file_name + \".bytes\"\n",
    "    all_file_words.append(get_words_for_document(file_name))\n",
    "idf = get_IDF(file_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "analyzed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = np.arange(256).astype(str)\n",
    "pd_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "tf_idf_df = df.from_pandas(pd_df, npartitions=1)\n",
    "\n",
    "for file_words in all_file_words:\n",
    "    tf_idf_array = get_TF_IDF(file_words, idf)\n",
    "    tf_idf_pandas = pd.DataFrame(data=tf_idf_array, columns=cols)\n",
    "    pandas_to_dask = df.from_pandas(tf_idf_pandas, npartitions=1)\n",
    "    tf_idf_df = tf_idf_df.append(pandas_to_dask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unknown-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = tf_idf_df.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
